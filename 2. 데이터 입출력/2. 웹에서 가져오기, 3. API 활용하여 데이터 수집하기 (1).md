# 웹(web)에서 가져오기
#### __1. HTML 웹 페이지에서 표 속성 가져오기__
##### 판다스 read_html() 함수는 HTML 웹 페이지에 있는 < table > 태그에서 표 형식의 데이터를 모두 찾아서 데이터프레임으로 변환한다. 표 데이터들은 각각 별도의 데이터프레임으로 변환되기 때문에 여러 개의 데이터프레임(표)을 원소로 갖는 리스트가 반환된다.
##### &nbsp;
##### HTML 표 속성을 읽어 데이터 프레임으로 변환하기 : pandas.rad_html("웹주소(URL)" 또는 "HTML 파일 경로(이름)")
```python
import pandas as pd

# HTML 파일 경로 or 웹 페이지 주소를 url 변수에 저장
url = r"C:\Users\구남이\Downloads\5674-833_4th\part2\sample.html"

# HTML 웹페이지의 표(table)를 가져와서 데이터프레임으로 변환 
tables = pd.read_html(url)

# 표(table)의 개수 확인
print(len(tables))
print('\n')

# tables 리스트의 원소를 iteration하면서 각각 화면 출력
for i in range(len(tables)):
    print("tables[%s]" % i)
    print(tables[i])
    print('\n')

# 파이썬 패키지 정보가 들어 있는 두 번째 데이터프레임을 선택하여 df 변수에 저장
df = tables[1] 

# 'name' 열을 인덱스로 지정
df.set_index(['name'], inplace=True)
print(df)
```
```
# 실행 결과
2


tables[0]
   Unnamed: 0  c0  c1  c2  c3
0           0   0   1   4   7
1           1   1   2   5   8
2           2   2   3   6   9

tables[1]
         name  year        developer  opensource
0       NumPy  2006  Travis Oliphant        True
1  matplotlib  2003   John D. Hunter        True
2      pandas  2008    Wes Mckinneye        True


            year        developer  opensource
name
NumPy       2006  Travis Oliphant        True
matplotlib  2003   John D. Hunter        True
pandas      2008    Wes Mckinneye        True
```
### &nbsp;
#### __2. 웹 스크래핑__
##### BeautifulSoup 등 웹 스크래핑 도구로 수집한 데이터를 판다스 데이터프레임으로 정리하는 방법 공부
##### 스크래핑한 내용을 파이썬 리스트, 딕셔너리 등으로 정리한 뒤 DataFrame( ) 함수에 리스트나 딕셔너리 형태로 전달하여 데이터프레임으로 변환한다.
##### &nbsp;
##### etfs[etf_ticker[0]] = [etf_market[0], etf_name[0]]와 같이 리스트를 원소로 갖는 딕셔너리를 정의해 딕셔너리를 데이터프레임으로 변환한다.
##### 왼쪽의 딕셔너리 키는 열 이름이 되고, 오른쪽 리스트는 열 데이터가 된다.
```python
# 라이브러리 불러오기
from bs4 import BeautifulSoup
import requests
import re
import pandas as pd

# 위키피디아 미국 ETF 웹 페이지에서 필요한 정보를 스크래핑하여 딕셔너리 형태로 변수 etfs에 저장
url = "https://en.wikipedia.org/wiki/List_of_American_exchange-traded_funds"
resp = requests.get(url)
soup = BeautifulSoup(resp.text, 'lxml')   
rows = soup.select('div > ul > li')
    
etfs = {}
for row in rows:
    
    try:
        etf_name = re.findall('^(.*) \(NYSE', row.text)
        etf_market = re.findall('\((.*)\|', row.text)
        etf_ticker = re.findall('NYSE Arca\|(.*)\)', row.text)
        
        if (len(etf_ticker) > 0) & (len(etf_market) > 0) & (len(etf_name) > 0):
            etfs[etf_ticker[0]] = [etf_market[0], etf_name[0]]

    except AttributeError as err:
        pass    

# etfs 딕셔너리 출력
print(etfs)
print('\n')

# etfs 딕셔너리를 데이터프레임으로 변환
df = pd.DataFrame(etfs)
print(df)
```
#### &nbsp;
> ##### 판다스 read_sql( ) 함수를 이용하면 SQL 쿼리를 가지고 데이터베이스로부터 데이터를 불러올 수 잇다. 이때 읽어온 데이터는 데이터프레임 포맷으로 저장된다.
#### &nbsp;
# API 활용하여 데이터 수집하기
##### 인터넷 서비스 업체에서 제공하는 API를 통해서 수집한 데이터를 판다스 자료구조로 변환한다.
##### 아래 코드에서 my_key 에 발급받은 API 키를 입력한다. 발급받은 API키는 외부에 유출되지 않도록 유의한다.
```python

# 라이브러리 가져오기
import googlemaps
import pandas as pd

my_key = "----발급받은 API 키를 입력-----"

# 구글맵스 객체 생성하기
maps = googlemaps.Client(key=my_key)  # my key값 입력

lat = []  #위도
lng = []  #경도

# 장소(또는 주소) 리스트
places = ["서울시청", "국립국악원", "해운대해수욕장"]

i=0
for place in places:   
    i = i + 1
    try:
        print(i, place)
        # 지오코딩 API 결과값 호출하여 geo_location 변수에 저장
        geo_location = maps.geocode(place)[0].get('geometry')
        lat.append(geo_location['location']['lat'])
        lng.append(geo_location['location']['lng'])
        
    except:
        lat.append('')
        lng.append('')
        print(i)

# 데이터프레임으로 변환하기
df = pd.DataFrame({'위도':lat, '경도':lng}, index=places)
print('\n')
print(df)
```
